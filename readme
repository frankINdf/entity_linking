entity linking
End-to-End Neural Entity Linking
Nikolaos Kolitsas, Octavian-Eugen Ganea, Thomas Hofmann
(Submitted on 23 Aug 2018 (v1), last revised 29 Aug 2018 (this version, v2))
一般方法是MD(mention detection)和ED(entity disambiguation),这篇文章提到了端到端的做法
所有可能是实体的span当成候选，计算和实体的文本相似度
关键点：context-aware mention embeddings, entity embeddings and a probabilistic mention - entity map
解决 [上海]俱乐部 这种问题
贡献：
1.需要的负例很少2.可以不用其他特征工程3.可迁移性
词向量 word2vec + char embedding
拼接首尾+soft_head(通过注意力得到)
为了表示非线性，增加了FFN


End-to-end Neural Coreference Resolution
第一篇端到端 指代消解论文
1.候选实体选取
consider all spans in a document as potential mentions and learn distributions over possible antecedents for each
2.边界获取
暴力法，得到N(N+1)/2
The model computes span embeddings that combine context-dependent boundary representations with a headfinding attention mechanism.
3.损失函数
maximize the marginal likelihood of gold antecedent spans from coreference clusters
挑战：复杂度高，使用了剪枝，剪枝方案：得到unary score后，选取lambda*N个,lambda0.4时是92%
每个span之间进行attention，得到span的权重X，concatstart\end\X\pusai 得到gi（qa问题会用到start\end）

4.成果
Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.

认为的不足
(1) parsing mistakes can introduce cascading errors and (2) many of the handengineered rules do not generalize to new languages.

Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking

问题
一个模型能不能做所有
Can all those steps be learned jointly with a model for contextualized text-representations, i.e. BERT
bert里面有多少信息
How much entity knowledge is already contained in pretrained BERT
额外的实体信息能否提升下游任务的表现
Does additional entity knowledge improve BERT’s performance in downstream tasks

初始化公开参数
and add an output classification layer on top of the architecture.
Given a contextualized token, the classifier computes the probability of an entity link for each entry in the entity vocabulary
转换为由词预测标签
cast it as a per token classification over the entire entity vocabulary
700k高频英文维基实体数据
训练数据来自英文维基百科
训练步骤
1.训练bert
first trained BERT-base-uncased on EnglishWikipedia (dubbed BERT+Entity)
2.在实体链接标准集上微调
fine-tuned and evaluated it on an entity linking benchmark

we do not have any supervision on mention-spans.An error analysis with validation data revealed that only 3% of errors are purely due
to span errors,while most errors are due to wrong Nil predictions which often coincided with entities being infrequent.
其他实验
1.固定bert的参数
Frozen-BERT+Entity is 6% below BERT+Entity, showing that BERT+Entity has learned additional entity knowledge.
2.额外实体信息能否提升结果
GLUE(语义理解) SQUAD(阅读理解) SWAG(阅读理解) EN-DE WMT(翻译)
neither helpful for the two QA datasets nor for machine translation, GLUE in which BERT+Entity improves 2%


we initialize BERT with the publicly available weights from the BERT-base-uncased model
add an output classification layer on top of the architecture
增加了一个分类层
Given a contextualized token, the classifier computes the probability of an entity link for each entry in the entity vocabulary
计算每个实体的概率
with jKBj being the number of entities in the KB
KB是entities的数量
ci是第i个的vec
p(j|v,h) the i-th token in context h — linking to entity j is computed by 激活函数
训练数据




































